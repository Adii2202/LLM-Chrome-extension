# To download and install Ollama, please visit the official website or GitHub repository. 
# After installation, open the command prompt and execute "ollama serve" to initiate the server, enabling the local execution of Mistral and LLama models. 
# Make sure to load the extension on the Chrome browser and activate it. 
# If an error occurs stating that port 11434 is occupied, you may need to terminate the process using that port and then rerun "ollama serve".
