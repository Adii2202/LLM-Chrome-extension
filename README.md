## Download and Install Ollama from the official website or github
## open command prompt and run "ollama run mistral" to run mistral and llama models locally
## we have used mistral here
## load the extension on the chrome extensions and run it.
## try running "ollama serve" to check the active port for ollama 
